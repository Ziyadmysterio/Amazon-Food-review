{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F21 AA. CW 1\n",
    "# TASK 3 Vector Space and Feature Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WILL BE ADDED AFTER ALL ALGORITMS RUN\n",
    "\n",
    "#FUNCTION TO DIPLAY BAR GRAPH OF TOP N GRAMS  FOR COMPARISON\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Score</th>\n",
       "      <th>Summary_text</th>\n",
       "      <th>length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>length_change</th>\n",
       "      <th>wordcount_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>good receiv product earli seller tastey great ...</td>\n",
       "      <td>103</td>\n",
       "      <td>16</td>\n",
       "      <td>104</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>organ kosher tasti assort premium tea teasan n...</td>\n",
       "      <td>1199</td>\n",
       "      <td>193</td>\n",
       "      <td>799</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>excel glutenfre spaghetti great tast great str...</td>\n",
       "      <td>619</td>\n",
       "      <td>101</td>\n",
       "      <td>527</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>lindt lindt buy multipack misl pictur whole ha...</td>\n",
       "      <td>117</td>\n",
       "      <td>19</td>\n",
       "      <td>84</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>yum bar good love warm definit think great sna...</td>\n",
       "      <td>88</td>\n",
       "      <td>14</td>\n",
       "      <td>76</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Score                                       Summary_text  \\\n",
       "0           0      5  good receiv product earli seller tastey great ...   \n",
       "1           1      5  organ kosher tasti assort premium tea teasan n...   \n",
       "2           2      5  excel glutenfre spaghetti great tast great str...   \n",
       "3           3      5  lindt lindt buy multipack misl pictur whole ha...   \n",
       "4           4      5  yum bar good love warm definit think great sna...   \n",
       "\n",
       "   length  word_count  length_change  wordcount_change  \n",
       "0     103          16            104                24  \n",
       "1    1199         193            799               126  \n",
       "2     619         101            527               103  \n",
       "3     117          19             84                15  \n",
       "4      88          14             76                15  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessed data from step 2 which used Stemming\n",
    "df_reviews_p_s = pd.read_csv('df_reviews_p_s.csv')\n",
    "df_reviews_p_s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Score</th>\n",
       "      <th>Summary_text</th>\n",
       "      <th>length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>length_change</th>\n",
       "      <th>wordcount_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>good received product early seller tastey grea...</td>\n",
       "      <td>109</td>\n",
       "      <td>16</td>\n",
       "      <td>98</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>organic kosher tasty assortment premium tea te...</td>\n",
       "      <td>1334</td>\n",
       "      <td>193</td>\n",
       "      <td>664</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>excellent glutenfree spaghetti great taste gre...</td>\n",
       "      <td>690</td>\n",
       "      <td>101</td>\n",
       "      <td>456</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>lindt lindt buying multipack misled picture wh...</td>\n",
       "      <td>138</td>\n",
       "      <td>19</td>\n",
       "      <td>63</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>yum bar good loved warmed definitely think gre...</td>\n",
       "      <td>98</td>\n",
       "      <td>14</td>\n",
       "      <td>66</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Score                                       Summary_text  \\\n",
       "0           0      5  good received product early seller tastey grea...   \n",
       "1           1      5  organic kosher tasty assortment premium tea te...   \n",
       "2           2      5  excellent glutenfree spaghetti great taste gre...   \n",
       "3           3      5  lindt lindt buying multipack misled picture wh...   \n",
       "4           4      5  yum bar good loved warmed definitely think gre...   \n",
       "\n",
       "   length  word_count  length_change  wordcount_change  \n",
       "0     109          16             98                24  \n",
       "1    1334         193            664               126  \n",
       "2     690         101            456               103  \n",
       "3     138          19             63                15  \n",
       "4      98          14             66                15  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessed data from step 2 which used Lemmatization\n",
    "df_reviews_p_l = pd.read_csv('df_reviews_p_l.csv')\n",
    "df_reviews_p_l.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Bag of Words - count vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using preprocesed with Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit data into bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer()\n",
    "vect.fit(df_reviews_p_s['Summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 166502\n",
      "Every 2000th feature:\n",
      "['00', '3xx', 'allicin', 'basel', 'bugspantri', 'chocodrop', 'crerncher', 'dontw', 'falland', 'fyiit', 'haymag', 'hrefhttpwwwamazoncomgpproductb001fa1dg4spectrum', 'itcreami', 'limeugh', 'milksemisweet', 'nonstevia', 'paleohealthi', 'preveg', 'reviewand', 'shelvesuntil', 'staneg', 'teascom', 'twistlock', 'welllet']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(\"Every 2000th feature:\\n{}\".format(feature_names[::7000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vect.transform(df_reviews_p_s['Summary_text'])\n",
    "Y_train=df_reviews_p_s['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X_Train: (426340, 166502)\n",
      "Size of Y_Train: (426340,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of X_Train: {}\".format(X_train.shape))\n",
    "print(\"Size of Y_Train: {}\".format(Y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(LogisticRegression(), X_train, Y_train, cv=5)\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using preprocesed with Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit data into bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "vect.fit(df_reviews_p_l['Summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 191651\n",
      "Every 2000th feature:\n",
      "['00', '39828', 'aged8years', 'awesomly', 'bonescute', 'carrol', 'coffeeseach', 'cutglass', 'dommage', 'excitd', 'formsits', 'grea', 'hosptials', 'httpwwwamazoncomgpproductb003tiyvpkrefwms_ohs_product', 'juciy', 'lollipopsthey', 'milknonfat', 'nom1', 'outdatedit', 'please', 'quck', 'rosesthx', 'shortagedisaster', 'squishiness', 'tannin', 'toulouse', 'verger', 'wrongthe']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(\"Every 7000th feature:\\n{}\".format(feature_names[::7000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vect.transform(df_reviews_p_l['Summary_text'])\n",
    "Y_train=df_reviews_p_l['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X_Train: (426340, 191651)\n",
      "Size of Y_Train: (426340,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of X_Train: {}\".format(X_train.shape))\n",
    "print(\"Size of Y_Train: {}\".format(Y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(LogisticRegression(), X_train, Y_train, cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.76\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) N-gram 2 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using preprocesed with Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit data into bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(2,2)).fit(df_reviews_p_s['Summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 3125304\n",
      "Every 2000th feature:\n",
      "['00 10', '125 felt', '1870 primari', '24 adult', '34th recip', '500 cool', '7g 2g', 'absorb sometim', 'activ herb', 'addict open', 'advis satisfi', 'ago mealtim', 'alimentum wife', 'alon sold', 'altern next', 'amazon hakubaku', 'anayway alway', 'anxieti never', 'apparantli good', 'area bottl', 'arriv vacuumpack', 'assum wonka', 'avail german', 'awesom nabisco', 'bacon smell', 'bait past', 'bar qualityi', 'bat mapl', 'beat price1', 'beguil amazon', 'besid bit', 'better splenda', 'biochem either', 'bitter groceri', 'blind robbin', 'boldbut littl', 'bottl hair', 'bowl start', 'brand extravirgin', 'breastfe result', 'broke squeez', 'buddi fed', 'burrito plain', 'buy pearson', 'cakey sort', 'came somehow', 'cannist keep', 'card go', 'case evian', 'caus stress', 'certainli variat', 'cheap amazingli', 'chef inspir', 'chicken overcook', 'chip vomit', 'cholesterol recheck', 'cinnamoni heavi', 'clearli goodfory', 'coat ingredi', 'coffe skim', 'color usual', 'common drink', 'complaint welcom', 'condit discov', 'constantli took', 'contracept yet', 'cooki peddler', 'cost honestli', 'coupl visit', 'crap legal', 'creme coconut', 'crunchier competit', 'curacao use', 'daili larg', 'daughter type', 'decad dark', 'definit hous', 'delish short', 'descript least', 'dha product', 'differ hesit', 'direct special', 'discrimin kitti', 'distributor doesnt', 'dollar gold', 'doug graham', 'dri slow', 'drown togeth', 'earlier edward', 'eat old', 'effect process', 'els furnitur', 'energ like', 'enough stopgap', 'especi help', 'even heard', 'everyon practic', 'except bbq', 'expens look', 'extract fl', 'faint heartbut', 'fantast interest', 'fathom anyon', 'feed feel', 'fibromyalgia result', 'find cv', 'firm seem', 'flake work', 'flavori havent', 'flu product', 'food mj', 'formula 99', 'fqavor product', 'fresh clean', 'frost flower', 'fullraw allnatur', 'garden brought', 'georgia pick', 'ghee world', 'giveaway damn', 'gnome isbn10', 'gone question', 'gooey chicken', 'grade matcha', 'grave unbeknownst', 'greatt common', 'groggi slight', 'guiltless pint', 'half definit', 'happen sale', 'hate brussel', 'healthi orchid', 'heavier commerci', 'hey begger', 'hit guy', 'honestli contact', 'hot better', 'howev split', 'human believ', 'icei ad', 'im san', 'inadequ descript', 'indistinguish krispi', 'ingredi select', 'instead nutrasweet', 'introduc gumbo', 'ita well', 'jalapenocheddar ole', 'jitter littl', 'junk melt', 'kept seizur', 'kind jampure', 'know disrupt', 'label confirm', 'lasagna wait', 'lba highli', 'left glum', 'let friend', 'light confect', 'like orrder', 'link salti', 'littl joke', 'locat uptown', 'look tray', 'love cod', 'lower ounc', 'mack toffe', 'major flavor', 'maluku island', 'margin help', 'matcha caramel', 'md drinker', 'medicin even', 'merci nonetheless', 'middleclass babi', 'min add', 'minut stow', 'mix uv', 'mom absolutli', 'morn job', 'movi still', 'multiyear search', 'name speed', 'necessit sprint', 'never hearti', 'nice pu', 'nonmeat part', 'noth chees', 'nut odor', 'object cut', 'offic flavorwis', 'okara add', 'one crisper', 'oogl coffe', 'order agit', 'origin cadburi', 'outsid fenc', 'overwhelm clove', 'packag east', 'paleo allnatur', 'part housewarm', 'pasteur chees', 'pear broccolilici', 'peppermint watermelon', 'perman post', 'pick enough', 'pinch ground', 'plant broken', 'plu 6th', 'pomegranatetangerin also', 'portion lesser', 'pound caraway', 'prefer fanci', 'pressur reduc', 'pricey greatr', 'proceed vomitingdiarrhea', 'product unadulter', 'protein comfort', 'pump great', 'purifi citi', 'qualiti store', 'quit star', 'raspberri freez', 'read job', 'reason exposur', 'recogn silver', 'refil necessari', 'rehydr product', 'remov fabric', 'research lowest', 'retail prompt', 'rice found', 'rip cant', 'rollup tend', 'rum 23', 'said oatswhich', 'salti suspicion', 'sauc classic', 'say equat', 'scoop wa', 'seawe snack', 'seem immun', 'send shown', 'server thai', 'shampoo past', 'shini also', 'shot 20', 'sighthound need', 'sinc news', 'size highbut', 'slightli doughi', 'smell fresher', 'snack tortilla', 'soft gentl', 'someth find', 'sorri coz', 'soybutt allerg', 'sphere bring', 'spoon command', 'squishi doar', 'starbuck inconsist', 'steak store', 'still float', 'storag least', 'strawberryseem multiberrycherri', 'stuf gummi', 'substitut dorito', 'sugarfre crystal', 'superior replac', 'sure wort', 'sweet spreadabl', 'syrup tang', 'taken common', 'tast expirement', 'tastier better', 'tealici treat', 'tend wear', 'thai also', 'thermo heat', 'thing thin', 'thoughalong aforement', 'thump nois', 'tin thought', 'togeth two', 'top drawer', 'town white', 'tray ad', 'tri hydrat', 'true kefir', 'turmer may', 'type campbel', 'understand crumbl', 'unlik stick', 'ur small', 'use nutrish', 'valu design', 'veget hate', 'vet shriek', 'vodka lighten', 'walnut moist', 'wasnt purpos', 'way crunch', 'week truck', 'wellround typic', 'whether breakfast', 'wife tempt', 'without bagel', 'worcestershir prawn', 'worst campbel', 'wrap thick', 'year 1star', 'yolk drop', 'yummo delish']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(\"Every 10,000th feature:\\n{}\".format(feature_names[::10000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vect.transform(df_reviews_p_s['Summary_text'])\n",
    "Y_train=df_reviews_p_s['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X_Train: (426340, 3125304)\n",
      "Size of Y_Train: (426340,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of X_Train: {}\".format(X_train.shape))\n",
    "print(\"Size of Y_Train: {}\".format(Y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(LogisticRegression(), X_train, Y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using preprocesed with Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit data into bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(2,2)).fit(df_reviews_p_l['Summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 3642796\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every 10,000th feature:\n",
      "['00 better', '1216 week', '180 empty', '23 mess', '334mg total', '4yearold become', '749 little', 'absolutely apricot', 'acidity overpowers', 'add ping', 'additive green', 'aeropress scoop', 'ago forgot', 'ale youll', 'almost everyway', 'also india', 'always plain', 'amazoncom van', 'animal corporate', 'anyone beef', 'appeal subscribe', 'apricot heshe', 'around tidy', 'asking fresh', 'attending humboldt', 'avoid investing', 'açaiacute juice', 'bad describe', 'baguette salami', 'bar 100', 'basic apple', 'bean capresso', 'bed infested', 'believe tazo', 'best fairest', 'betteri poured', 'biodegradable use', 'biting chai', 'blend lived', 'bodied buying', 'booster calorie', 'bought colombiai', 'box numi', 'brand seed', 'breath lifted', 'brittle little', 'buck cheaper', 'bunny received', 'button kneecap', 'ca sample', 'call newman', 'camper cabin', 'canned spaghettio', 'carbonation instead', 'carry better', 'cat novel', 'cellphane bag', 'challange gave', 'cheaper 10lbs', 'chef rasoi', 'chia actually', 'chinese reasturant', 'chocolatey hippo', 'chunk can', 'classic every', 'close ordinary', 'coco 17oz', 'coffee tire', 'color yeast', 'coming anyone', 'compared buttertype', 'compliment mix', 'condo early', 'consists bitesized', 'contains anchovy', 'conventional water', 'cooky go', 'cost 5065bar', 'countless vet', 'cracker done', 'creamer looking', 'croix house', 'cube also', 'current medium', 'dairy kind', 'daughter saysdelicious', 'dealer though', 'deep trouble', 'delicious hrefhttpwwwamazoncomgpproductb001e5e2rctorani', 'dense product', 'desirable theyve', 'diabetic little', 'difference muffin', 'dinner random', 'disappointing zevia', 'dish shocked', 'dobut msg', 'dollop torani', 'doubt pepper', 'dried mint', 'driving truck', 'dump solid', 'easily invade', 'eater looking', 'effect diminish', 'electronic scale', 'encountered sterling', 'enjoy hrefhttpwwwamazoncomgpproductb0006i8j5emackinac', 'enter middle', 'especially overweight', 'even essentia', 'every litter', 'example redbull', 'exhausted sink', 'experience delayed', 'extract compare', 'factual journalist', 'fan primal', 'faster edit', 'favorite whiskas', 'feline son', 'fill buster', 'find mainland', 'fire ingredient', 'fixed hot', 'flavor pickled', 'flax hence', 'flytrap inch', 'food motivateduntil', 'form purina', 'found stirring', 'freesia tropical', 'friend fantastic', 'fruity pasta', 'furikake variety', 'gassysimilac make', 'germany previous', 'getting six', 'give feral', 'glucosamine help', 'god soooo', 'good formula', 'goraw perfect', 'graham substantial', 'gravity tried', 'greate beverage', 'grit havent', 'guavapolitian tasted', 'gym smoothy', 'hand guesswork', 'happy rid', 'havei going', 'healthy delectable', 'heavier try', 'herb side', 'highway experiencing', 'home barn', 'hope cure', 'hotter hottest', 'hrefhttpwwwamazoncomgpproductb0006onqoccuisinart ice30bc', 'humor rather', 'icing left', 'im got', 'impressed bitter', 'incorporate vitamin', 'infamous charley', 'initially food', 'instense flavor', 'intolerant used', 'issue coincidence', 'ive kcup', 'jello thats', 'jug still', 'kcups friday', 'key actually', 'kinda day', 'know immediately', 'lab substance', 'large teabags', 'law happy', 'leak used', 'leftover packet', 'liberally spread', 'lightly chewed', 'like rib', 'line basic', 'listing im', 'lived gave', 'long dry', 'looking tradional', 'love beaver', 'loved word', 'lunch hoping', 'made quick', 'make grilled', 'malty year', 'mar board', 'mashed spread', 'maybe emerald', 'measure level', 'melt blend', 'messy bought', 'might remind', 'mind cat', 'minute sometimes', 'mix pleasant', 'mointain dew', 'montgomery alabamaa', 'mother sharing', 'much differs', 'mushroom black', 'napolitans milk', 'necessary others', 'nescafe classico', 'next propylene', 'no total', 'normal major', 'noticeably energized', 'nutitious easy', 'objectionable tangy', 'offered looked', 'oiled skillet', 'olive thinking', 'one vsop', 'opening touching', 'order reply', 'organic stop', 'ounce shop', 'overpackaged little', 'pack 6much', 'packaging shine', 'pale wan', 'part champagne', 'pasta interacted', 'pea satisfying', 'people mysteriously', 'perfect suitable', 'pet also', 'picky greyhound', 'pinkish chicken', 'plant dull', 'pleasure nice', 'pointing ill', 'popper far', 'pot larger', 'powder handiest', 'preferidas bean', 'press digital', 'price revised', 'probably spoiled', 'product asinb002bg38r8', 'professional wont', 'protein price', 'pulverized inner', 'purchased soy', 'put towel', 'questo motivo', 'rained especially', 'rather convententional', 'reader interested', 'really tasted', 'recently sitting', 'recommendedand gluten', 'refreshment includes', 'rejoice using', 'reminds id', 'reprimanded falsely', 'responsible mishandling', 'reunited keurig', 'rice kasmati', 'ringer certain', 'rocked flavor', 'routine made', 'sad favourite', 'salmon idea', 'sampling tea', 'sauce reek', 'say essentially', 'science center', 'searching banana', 'see microwave', 'seen clipper', 'senf tasted', 'served tuna', 'sfuff cup', 'shelf finally', 'shipping note', 'show lump', 'significantly find', 'since pancake', 'size 112', 'sleeve first', 'small salty', 'smooth foam', 'sniff taste', 'softness pliability', 'something read', 'sorrow dismay', 'sowed directly', 'spectrum isnt', 'splash color', 'sprinkle ok', 'stand delicious', 'start tuna', 'stealer freshness', 'still 11', 'stomach totally', 'stove apple', 'strong depth', 'stuffer wrap', 'success know', 'sugary crispy', 'superior design', 'sure fox', 'swear first', 'sweetness seems', 'table carmel', 'talk mini', 'taste fanta', 'tastier salt', 'tea county', 'tedious journey', 'teriyakiflavored one', 'thai relative', 'there seems', 'thing drinkthat', 'thirdparty merchant', 'threeyear old', 'time backfired', 'tip main', 'told meant', 'top garnish', 'tour croatiaslovenia', 'translated little', 'treck town', 'trip make', 'try gourmetfoodstore', 'tuna stew', 'two eat', 'unacceptable worst', 'unheard previous', 'unused bathroom', 'use carbon', 'used spoiled', 'utilitarian aesthetically', 'variety unreal', 'vermicelli said', 'vinny witness', 'waffle tasted', 'want throw', 'wassail aroma', 'watery normally', 'website touted', 'well drew', 'weve hairball', 'white machine', 'wine added', 'without scissors', 'worcestershire page', 'worn simicrushed', 'would taken', 'wyler mr', 'yes admit', 'youre lowcaf', 'zico girlfriend']\n"
     ]
    }
   ],
   "source": [
    "print(\"Every 10,000th feature:\\n{}\".format(feature_names[::10000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vect.transform(df_reviews_p_l['Summary_text'])\n",
    "Y_train=df_reviews_p_l['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X_Train: (426340, 3642796)\n",
      "Size of Y_Train: (426340,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of X_Train: {}\".format(X_train.shape))\n",
    "print(\"Size of Y_Train: {}\".format(Y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(LogisticRegression(), X_train, Y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) N-gram 1-2 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using preprocesed with Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit data into bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2)).fit(df_reviews_p_s['Summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 3291806\n",
      "Every 10,000th feature:\n",
      "['00', '12 hawaiian', '160 york', '2010 wrapper', '2999 plu', '3oz hard', '55 cs', '7lb kibbl', 'absolut amazingfor', 'acronym limit', 'add pineappl', 'advantag protect', 'agav freshli', 'al solut', 'almond crunchi', 'also could', 'alway stinki', 'american fanta', 'annoy knot', 'anyth gusto', 'appl dog', 'arguabl product', 'artifici sweetn', 'ate meantim', 'availabilityi', 'awhil actual', 'baconthen', 'bailey love', 'bar land', 'basket convert', 'bear glad', 'beforeand', 'benic chewycouldnt', 'better constip', 'big veget', 'bit radishi', 'blaot 30', 'bluepurpl powder', 'bookstor austin', 'bought coupl', 'box stinki', 'brave entir', 'brew maker', 'brother regan', 'build cheaper', 'busi think', 'buy tester', 'calbe', 'came stair', 'canist think', 'carbon consid', 'carton altern', 'catsup either', 'cereal relief', 'chappi less', 'chees conesiur', 'chewabl top', 'chiliit', 'chocol scratch', 'christma raspberri', 'clapshot go', 'close leak', 'cocoa perform', 'coffeeso much', 'combin damag', 'compani calcium', 'complet matter', 'condition either', 'construct dont', 'contrari vegetarian', 'cooki posit', 'cost 679', 'coupl blast', 'cram cluster', 'creat punch', 'crumbspanko fri', 'cup mach', 'cute love', 'darker milk', 'dd mickeyd', 'deck 78', 'delici beefi', 'denser nabisco', 'despit fine', 'dictionari would', 'difficulti pass', 'disappear cover', 'dish care', 'dn tast', 'domin menu', 'doughnut day', 'dri uncook', 'drown cream', 'ear smokehous', 'eat gummybear', 'edrignton group', 'eleg style', 'end coplaint', 'enjoy trust', 'equilibrium weight', 'etern quest', 'ever special', 'exactli plant', 'exist realli', 'expir shelf', 'fabul small', 'famili dine', 'farm decid', 'favorit catalina', 'feel pumpkin', 'figur make', 'find peppermint', 'first italian', 'flat open', 'flavorsroast bold', 'fluff omph', 'food meet', 'format want', 'four hr', 'french roastflavor', 'frise havanes', 'full brew', 'gaga spici', 'gelato singl', 'get overload', 'girl cocktail', 'glorifi candybar', 'gobbl hope', 'good kataifi', 'got kept', 'gram pouch', 'great cet', 'greeni recent', 'ground whitefish', 'gum taffe', 'half tablspoon', 'happi bubbl', 'hate echo', 'healthi lollipop', 'heavi fine', 'hero candi', 'hint past', 'homebrew bit', 'horrend mess', 'hous worst', 'hrefhttpwwwamazoncomgpproductb0017y9ukonabisco', 'hull rice', 'ice lose', 'im forc', 'impress yet', 'indi', 'ingredi ad', 'instanc noodl', 'interestingli come', 'isnt fare', 'itselfil', 'java crazi', 'joy given', 'kcup bzzagent', 'keylimetim', 'kirkland ad', 'know tini', 'labl front', 'last monthi', 'lead faster', 'left moutho', 'let downand', 'lift mood', 'like mission', 'liner compost', 'littl chocol', 'local grate', 'look gait', 'lotteri someth', 'lover top', 'lyche altern', 'magnesium almond', 'make versitl', 'mani true', 'marri anoth', 'may edit', 'mean tini', 'melinda ghost', 'messi nonpoopinduc', 'might win', 'mind sesam', 'mirin probabl', 'mixin right', 'mom cousin', 'morn cocoa', 'move path', 'mugsi', 'nacho know', 'near regist', 'nephew cousin', 'next opportun', 'noh poki', 'normal ito', 'notwithstand seem', 'nutrit point', 'occas sometim', 'often schedul', 'old aspi', 'one hrefhttpwwwamazoncomgpproductb000e3vamkzatarain', 'open 25lb', 'order breakdown', 'origin christma', 'outrag problem', 'oversight inform', 'pack welltoo', 'painkil found', 'parent germani', 'past melitta', 'pea lemon', 'peopl metabol', 'perfectli arent', 'petrifi', 'pie local', 'pizza mediocr', 'platinum b70', 'pocket high', 'pop bake', 'possibl salad', 'pour miser', 'prefer tea', 'pretti face', 'prici store', 'process diet', 'product voluntarili', 'protect transit', 'pull stick', 'pure brocolli', 'qualiti coars', 'quik open', 'rancid unopen', 'rawhid origin', 'realli contact', 'recent relax', 'recycl yet', 'regarless', 'remain dress', 'replac wish', 'respons truth', 'review butler', 'ricochet brand', 'rn need', 'root tree', 'run surpris', 'salad crush', 'sam choic', 'sauc figur', 'say faint', 'scoop essenti', 'season tomato', 'seek seriou', 'seltzerlik', 'serv closest', 'shake dish', 'shell pick', 'shop cherrybrook', 'sibl actual', 'simpl make', 'sit assist', 'skipjack mayb', 'small advis', 'smoki tang', 'snob found', 'soil ugli', 'sometim joint', 'soso half', 'spaghetti use', 'spice option', 'spoon total', 'sriracha still', 'starbuck half', 'steak definit', 'still 1232', 'stop diseas', 'stranger world', 'struggl water', 'subscrib love', 'sugar deathlysweet', 'sundaysi usual', 'suppos odor', 'swallow give', 'swirl water', 'taco night', 'tannin tast', 'tast tamal', 'tea add', 'techniqu robust', 'terminolog bought', 'thank meltingmama', 'theyll left', 'think admit', 'thought bite', 'thun honey', 'tin health', 'togeth cat', 'tooth order', 'toughest criticstwo', 'trap cat', 'tri appreci', 'trip trap', 'tugajug two', 'twizzler letter', 'um thank', 'unfound person', 'unsuspect windpip', 'usa ice', 'useda', 'vanilla tone', 'veggi swim', 'video addit', 'volum tast', 'wanna therefor', 'wasnt syrupi', 'way cheesey', 'week person', 'wellb', 'wheatgrass year', 'whose afternoon', 'wise wise', 'wonder retain', 'world godiva', 'would thwart', 'xray either', 'yellow squash', 'youv crystal', 'zip zero']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(\"Every 10,000th feature:\\n{}\".format(feature_names[::10000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vect.transform(df_reviews_p_s['Summary_text'])\n",
    "Y_train=df_reviews_p_s['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X_Train: (426340, 3291806)\n",
      "Size of Y_Train: (426340,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of X_Train: {}\".format(X_train.shape))\n",
    "print(\"Size of Y_Train: {}\".format(Y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(LogisticRegression(), X_train, Y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using preprocesed with Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit data into bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2)).fit(df_reviews_p_l['Summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 3834447\n",
      "Every 10,000th feature:\n",
      "['00', '12 crushed', '16 175', '2008 resulting', '28 free', '392ounce', '50for 61its', '73112', 'able chance', 'ache digestive', 'ad first', 'adding sparkling', 'advertised 12', 'afterwards supply', 'air sorry', 'allliquid', 'along splenda', 'alternative healthier', 'amazing threeyear', 'among spicy', 'another cat', 'anything frosting', 'appeared usually', 'arching green', 'arrival dont', 'asks argue', 'attempted pull', 'avilalble 10', 'awhile hit', 'bacon first', 'bag uncle', 'balsamics ok', 'barrier effective', 'batter uniform', 'beautiful practical', 'begin producing', 'benot true', 'better airflow', 'big 500', 'bisulfate review', 'bjs one', 'blight', 'boil leapt', 'boring spending', 'bought lindt', 'box squeeze', 'brand ultimate', 'breath provide', 'british sunday', 'bubble dammit', 'bunch along', 'butter swirl', 'buying rock', 'cal drive', 'cals cant', 'candy pleasant', 'captain coke', 'careful pouch', 'case load', 'cause cell', 'cereal nondiscount', 'change ro', 'check kcup', 'cherry came', 'chicken packaging', 'chip individual', 'chocoperfection time', 'chunk desire', 'class night', 'close 78', 'coating taste', 'coffee owned', 'collie took', 'come layer', 'company pod', 'complement would', 'concerning classic', 'considerably love', 'contain almond', 'contiues go', 'cookie addictive', 'cordial little', 'couch cure', 'course hot', 'cranberry salmon', 'create white', 'crumb puppy', 'cul nut', 'curry many', 'dalfours nosugar', 'daughter steamed', 'deal typically', 'dedicated milk', 'delicious banana', 'delivery wife', 'description specifically', 'development make', 'diet kuma', 'digestion key', 'disagree first', 'discovered patella', 'dissipates chocolate', 'dog euthanized', 'dont large', 'dr earl', 'drink loose', 'droste depending', 'dust fart', 'easy always', 'eating dreamfields', 'effect sought', 'element chewy', 'encompassing complex', 'enjoy choose', 'enoughif youre', 'esp cold', 'evacuated', 'ever pot', 'ew sounded', 'exchange update', 'expend carb', 'exposed environment', 'face picky', 'familial tendency', 'far person', 'fault needed', 'feed child', 'fevertree consider', 'filling mini', 'finding need', 'first impact', 'flaky butter', 'flavor untill', 'flimsly container', 'fod love', 'food prior', 'form rutile', 'found shelled', 'free tooo', 'fridge unfortunately', 'fruit ripe', 'fun show', 'garden store', 'generic fruit', 'get sixpack', 'ginger bud', 'glacier every', 'go attempt', 'goldendoodle patient', 'good successful', 'got sooo', 'grand turk', 'great boought', 'green insideout', 'grocery twenty', 'guessed good', 'habit id', 'hand monday', 'happy searched', 'hatehatehate microwavable', 'healthiest type', 'heaven accepted', 'helping tummy', 'higher avail', 'hold sort', 'honey mostly', 'horseradish1', 'hover immediately', 'hrefhttpwwwamazoncomgpproductb001crawcqilly', 'human foodmeat', 'ice black', 'illegal last', 'implies yet', 'included strawberry', 'individual metal', 'ingredient disqualify', 'inspired super', 'intention sounded', 'ireland spending', 'itchy', 'jalepenolol love', 'jersey taste', 'juice spill', 'keaton say', 'kibble bad', 'kinda shape', 'know gooseberry', 'ky diagnosed', 'largas', 'latte still', 'leaf bread', 'led order', 'let would', 'lifetime curse', 'like hypertension', 'likely worst', 'liquid placing', 'little lozenge', 'localhee hee', 'look deliscious', 'lot assure', 'love office', 'lowcal snack', 'macadamia several', 'magma ready', 'make stopped', 'maneuvering', 'marinade basically', 'massive trembling', 'maybe pick', 'measured make', 'mellow soy', 'mess soda', 'midst', 'mill shredded', 'mint tone', 'mistitled dont', 'mmmmm texture', 'moment relax', 'morning earthy', 'mouththroat long', 'mucusy liquid', 'mustard bad', 'natural grocersvitamin', 'need equivalent', 'never adverse', 'nice book', 'noise something', 'normal floor', 'notice total', 'nut noodle', 'oatmeal container', 'odor tell', 'oil coom', 'old nasty', 'one latter', 'oomph tea', 'optionjack', 'ordering finished', 'orleans miss', 'outstanding buy', 'overthemoon woodgrain', 'pack theyre', 'packet divided', 'pan package', 'part pulled', 'pastacheesenew', 'peaceofminddestroyers', 'people liar', 'perfect purchase', 'pertain toy', 'pickier one', 'pilot hoping', 'plain onion', 'please assured', 'pod emptyits', 'pop appreciated', 'positive quick', 'pouchesall themi', 'prairie matched', 'prepare enjoyed', 'pretzel hrefhttpwwwamazoncomgpproductb000g7tc38snyders', 'pricey toasted', 'problem rat', 'product granule', 'prolactin', 'proven unsurpassed', 'pumpkin crop', 'purchasing caused', 'putting carnation', 'questioned iodine', 'rage', 'rate may', 'read case', 'really item', 'received turned', 'recommend wholey', 'refill cap', 'regular option', 'remarkably quick', 'replace plus', 'resemblance whatsoever', 'result roasting', 'reviewed enjoy', 'rid salt', 'rit havent', 'roo ordered', 'rugrats type', 'safely recommended', 'salt activated', 'sandwich mixing', 'sauce thinly', 'say gary', 'school uk', 'search food', 'see christmas', 'seems justice', 'semimashed style', 'seriously plasticlike', 'seven newer', 'shared uh', 'shipment end', 'short travel', 'sickeningly', 'simple really', 'sipping whiskey', 'skeptical starbucks', 'slightly teaflavored', 'smell barbecue', 'smores awful', 'soaking raw', 'sold kashi', 'sometimes decaf', 'sort fakey', 'soy soyallergic', 'speed 62', 'spite year', 'spring poland', 'stale oat', 'starsbutthey taste', 'stay trader', 'stick trader', 'stock remember', 'store scoop', 'stress alimony', 'stuff biscuit', 'subsitutei', 'sugar combo', 'summary would', 'supplier food', 'surprise involved', 'sweet reconstituted', 'switched drop', 'taco salsa', 'tandoori chicken', 'taste marvelous', 'tasting four', 'tea filled', 'teeccino thought', 'terephthalate also', 'texturelook piece', 'theory amount', 'thin german', 'thinking possibly', 'thought ziti', 'tiding', 'timothy others', 'toddler house', 'tonsil wait', 'tossed thats', 'tract many', 'tread shopping', 'tried brandflavor', 'true hit', 'tryed gloria', 'turn pro', 'twodays', 'unbreakable packaging', 'unique almond', 'unused still', 'use b60', 'used roughly', 'usually sold', 'variety healthwise', 'vendor intended', 'viewed drink', 'volume volume', 'walnut half', 'warning tooth', 'water litre', 'wayne utah', 'weigh 1215', 'wellliked give', 'wheat yeah', 'wholefoods sell', 'winter made', 'wolfsize', 'work associated', 'worry tummy', 'wouldnt without', 'xp excuse', 'yes 5g', 'youre certain', 'zero doesnt']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(\"Every 10,000th feature:\\n{}\".format(feature_names[::10000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vect.transform(df_reviews_p_l['Summary_text'])\n",
    "Y_train=df_reviews_p_l['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X_Train: (426340, 3834447)\n",
      "Size of Y_Train: (426340,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of X_Train: {}\".format(X_train.shape))\n",
    "print(\"Size of Y_Train: {}\".format(Y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(LogisticRegression(), X_train, Y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) TF-IDF vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using preprocesed with Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit data into bag of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_TfIdf = TfidfVectorizer()\n",
    "vect_TfIdf.fit(df_reviews_p_s['Summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 166502\n",
      "Every 7000th feature:\n",
      "['00', '3xx', 'allicin', 'basel', 'bugspantri', 'chocodrop', 'crerncher', 'dontw', 'falland', 'fyiit', 'haymag', 'hrefhttpwwwamazoncomgpproductb001fa1dg4spectrum', 'itcreami', 'limeugh', 'milksemisweet', 'nonstevia', 'paleohealthi', 'preveg', 'reviewand', 'shelvesuntil', 'staneg', 'teascom', 'twistlock', 'welllet']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect_TfIdf.get_feature_names()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(\"Every 7000th feature:\\n{}\".format(feature_names[::7000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=vect_TfIdf.transform(df_reviews_p_s['Summary_text'])\n",
    "Y_train=df_reviews_p_s['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X_Train: (426340, 166502)\n",
      "Size of Y_Train: (426340,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of X_Train: {}\".format(X_train.shape))\n",
    "print(\"Size of Y_Train: {}\".format(Y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(LogisticRegression(), X_train, Y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using preprocesed with Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit data into bag of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_TfIdf = TfidfVectorizer()\n",
    "vect_TfIdf.fit(df_reviews_p_l['Summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 191651\n",
      "Every 7000th feature:\n",
      "['00', '39828', 'aged8years', 'awesomly', 'bonescute', 'carrol', 'coffeeseach', 'cutglass', 'dommage', 'excitd', 'formsits', 'grea', 'hosptials', 'httpwwwamazoncomgpproductb003tiyvpkrefwms_ohs_product', 'juciy', 'lollipopsthey', 'milknonfat', 'nom1', 'outdatedit', 'please', 'quck', 'rosesthx', 'shortagedisaster', 'squishiness', 'tannin', 'toulouse', 'verger', 'wrongthe']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect_TfIdf.get_feature_names()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(\"Every 7000th feature:\\n{}\".format(feature_names[::7000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Transform data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=vect_TfIdf.transform(df_reviews_p_l['Summary_text'])\n",
    "Y_train=df_reviews_p_l['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X_Train: (426340, 191651)\n",
      "Size of Y_Train: (426340,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of X_Train: {}\".format(X_train.shape))\n",
    "print(\"Size of Y_Train: {}\".format(Y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Run Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(LogisticRegression(), X_train, Y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) N-gram 1-2 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using preprocesed with Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit data into bag of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_TfIdf = TfidfVectorizer(ngram_range=(1,2)).fit(df_reviews_p_s['Summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 3291806\n",
      "Every 10,000th feature:\n",
      "['00', '12 hawaiian', '160 york', '2010 wrapper', '2999 plu', '3oz hard', '55 cs', '7lb kibbl', 'absolut amazingfor', 'acronym limit', 'add pineappl', 'advantag protect', 'agav freshli', 'al solut', 'almond crunchi', 'also could', 'alway stinki', 'american fanta', 'annoy knot', 'anyth gusto', 'appl dog', 'arguabl product', 'artifici sweetn', 'ate meantim', 'availabilityi', 'awhil actual', 'baconthen', 'bailey love', 'bar land', 'basket convert', 'bear glad', 'beforeand', 'benic chewycouldnt', 'better constip', 'big veget', 'bit radishi', 'blaot 30', 'bluepurpl powder', 'bookstor austin', 'bought coupl', 'box stinki', 'brave entir', 'brew maker', 'brother regan', 'build cheaper', 'busi think', 'buy tester', 'calbe', 'came stair', 'canist think', 'carbon consid', 'carton altern', 'catsup either', 'cereal relief', 'chappi less', 'chees conesiur', 'chewabl top', 'chiliit', 'chocol scratch', 'christma raspberri', 'clapshot go', 'close leak', 'cocoa perform', 'coffeeso much', 'combin damag', 'compani calcium', 'complet matter', 'condition either', 'construct dont', 'contrari vegetarian', 'cooki posit', 'cost 679', 'coupl blast', 'cram cluster', 'creat punch', 'crumbspanko fri', 'cup mach', 'cute love', 'darker milk', 'dd mickeyd', 'deck 78', 'delici beefi', 'denser nabisco', 'despit fine', 'dictionari would', 'difficulti pass', 'disappear cover', 'dish care', 'dn tast', 'domin menu', 'doughnut day', 'dri uncook', 'drown cream', 'ear smokehous', 'eat gummybear', 'edrignton group', 'eleg style', 'end coplaint', 'enjoy trust', 'equilibrium weight', 'etern quest', 'ever special', 'exactli plant', 'exist realli', 'expir shelf', 'fabul small', 'famili dine', 'farm decid', 'favorit catalina', 'feel pumpkin', 'figur make', 'find peppermint', 'first italian', 'flat open', 'flavorsroast bold', 'fluff omph', 'food meet', 'format want', 'four hr', 'french roastflavor', 'frise havanes', 'full brew', 'gaga spici', 'gelato singl', 'get overload', 'girl cocktail', 'glorifi candybar', 'gobbl hope', 'good kataifi', 'got kept', 'gram pouch', 'great cet', 'greeni recent', 'ground whitefish', 'gum taffe', 'half tablspoon', 'happi bubbl', 'hate echo', 'healthi lollipop', 'heavi fine', 'hero candi', 'hint past', 'homebrew bit', 'horrend mess', 'hous worst', 'hrefhttpwwwamazoncomgpproductb0017y9ukonabisco', 'hull rice', 'ice lose', 'im forc', 'impress yet', 'indi', 'ingredi ad', 'instanc noodl', 'interestingli come', 'isnt fare', 'itselfil', 'java crazi', 'joy given', 'kcup bzzagent', 'keylimetim', 'kirkland ad', 'know tini', 'labl front', 'last monthi', 'lead faster', 'left moutho', 'let downand', 'lift mood', 'like mission', 'liner compost', 'littl chocol', 'local grate', 'look gait', 'lotteri someth', 'lover top', 'lyche altern', 'magnesium almond', 'make versitl', 'mani true', 'marri anoth', 'may edit', 'mean tini', 'melinda ghost', 'messi nonpoopinduc', 'might win', 'mind sesam', 'mirin probabl', 'mixin right', 'mom cousin', 'morn cocoa', 'move path', 'mugsi', 'nacho know', 'near regist', 'nephew cousin', 'next opportun', 'noh poki', 'normal ito', 'notwithstand seem', 'nutrit point', 'occas sometim', 'often schedul', 'old aspi', 'one hrefhttpwwwamazoncomgpproductb000e3vamkzatarain', 'open 25lb', 'order breakdown', 'origin christma', 'outrag problem', 'oversight inform', 'pack welltoo', 'painkil found', 'parent germani', 'past melitta', 'pea lemon', 'peopl metabol', 'perfectli arent', 'petrifi', 'pie local', 'pizza mediocr', 'platinum b70', 'pocket high', 'pop bake', 'possibl salad', 'pour miser', 'prefer tea', 'pretti face', 'prici store', 'process diet', 'product voluntarili', 'protect transit', 'pull stick', 'pure brocolli', 'qualiti coars', 'quik open', 'rancid unopen', 'rawhid origin', 'realli contact', 'recent relax', 'recycl yet', 'regarless', 'remain dress', 'replac wish', 'respons truth', 'review butler', 'ricochet brand', 'rn need', 'root tree', 'run surpris', 'salad crush', 'sam choic', 'sauc figur', 'say faint', 'scoop essenti', 'season tomato', 'seek seriou', 'seltzerlik', 'serv closest', 'shake dish', 'shell pick', 'shop cherrybrook', 'sibl actual', 'simpl make', 'sit assist', 'skipjack mayb', 'small advis', 'smoki tang', 'snob found', 'soil ugli', 'sometim joint', 'soso half', 'spaghetti use', 'spice option', 'spoon total', 'sriracha still', 'starbuck half', 'steak definit', 'still 1232', 'stop diseas', 'stranger world', 'struggl water', 'subscrib love', 'sugar deathlysweet', 'sundaysi usual', 'suppos odor', 'swallow give', 'swirl water', 'taco night', 'tannin tast', 'tast tamal', 'tea add', 'techniqu robust', 'terminolog bought', 'thank meltingmama', 'theyll left', 'think admit', 'thought bite', 'thun honey', 'tin health', 'togeth cat', 'tooth order', 'toughest criticstwo', 'trap cat', 'tri appreci', 'trip trap', 'tugajug two', 'twizzler letter', 'um thank', 'unfound person', 'unsuspect windpip', 'usa ice', 'useda', 'vanilla tone', 'veggi swim', 'video addit', 'volum tast', 'wanna therefor', 'wasnt syrupi', 'way cheesey', 'week person', 'wellb', 'wheatgrass year', 'whose afternoon', 'wise wise', 'wonder retain', 'world godiva', 'would thwart', 'xray either', 'yellow squash', 'youv crystal', 'zip zero']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect_TfIdf.get_feature_names()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(\"Every 10,000th feature:\\n{}\".format(feature_names[::10000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Transform data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=vect_TfIdf.transform(df_reviews_p_s['Summary_text'])\n",
    "Y_train=df_reviews_p_s['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X_Train: (426340, 3291806)\n",
      "Size of Y_Train: (426340,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of X_Train: {}\".format(X_train.shape))\n",
    "print(\"Size of Y_Train: {}\".format(Y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Run Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(LogisticRegression(), X_train, Y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.79\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using preprocesed with Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit data into bag of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_TfIdf = TfidfVectorizer(ngram_range=(1,2)).fit(df_reviews_p_l['Summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 3834447\n",
      "Every 1,000th feature:\n",
      "['00', '12 crushed', '16 175', '2008 resulting', '28 free', '392ounce', '50for 61its', '73112', 'able chance', 'ache digestive', 'ad first', 'adding sparkling', 'advertised 12', 'afterwards supply', 'air sorry', 'allliquid', 'along splenda', 'alternative healthier', 'amazing threeyear', 'among spicy', 'another cat', 'anything frosting', 'appeared usually', 'arching green', 'arrival dont', 'asks argue', 'attempted pull', 'avilalble 10', 'awhile hit', 'bacon first', 'bag uncle', 'balsamics ok', 'barrier effective', 'batter uniform', 'beautiful practical', 'begin producing', 'benot true', 'better airflow', 'big 500', 'bisulfate review', 'bjs one', 'blight', 'boil leapt', 'boring spending', 'bought lindt', 'box squeeze', 'brand ultimate', 'breath provide', 'british sunday', 'bubble dammit', 'bunch along', 'butter swirl', 'buying rock', 'cal drive', 'cals cant', 'candy pleasant', 'captain coke', 'careful pouch', 'case load', 'cause cell', 'cereal nondiscount', 'change ro', 'check kcup', 'cherry came', 'chicken packaging', 'chip individual', 'chocoperfection time', 'chunk desire', 'class night', 'close 78', 'coating taste', 'coffee owned', 'collie took', 'come layer', 'company pod', 'complement would', 'concerning classic', 'considerably love', 'contain almond', 'contiues go', 'cookie addictive', 'cordial little', 'couch cure', 'course hot', 'cranberry salmon', 'create white', 'crumb puppy', 'cul nut', 'curry many', 'dalfours nosugar', 'daughter steamed', 'deal typically', 'dedicated milk', 'delicious banana', 'delivery wife', 'description specifically', 'development make', 'diet kuma', 'digestion key', 'disagree first', 'discovered patella', 'dissipates chocolate', 'dog euthanized', 'dont large', 'dr earl', 'drink loose', 'droste depending', 'dust fart', 'easy always', 'eating dreamfields', 'effect sought', 'element chewy', 'encompassing complex', 'enjoy choose', 'enoughif youre', 'esp cold', 'evacuated', 'ever pot', 'ew sounded', 'exchange update', 'expend carb', 'exposed environment', 'face picky', 'familial tendency', 'far person', 'fault needed', 'feed child', 'fevertree consider', 'filling mini', 'finding need', 'first impact', 'flaky butter', 'flavor untill', 'flimsly container', 'fod love', 'food prior', 'form rutile', 'found shelled', 'free tooo', 'fridge unfortunately', 'fruit ripe', 'fun show', 'garden store', 'generic fruit', 'get sixpack', 'ginger bud', 'glacier every', 'go attempt', 'goldendoodle patient', 'good successful', 'got sooo', 'grand turk', 'great boought', 'green insideout', 'grocery twenty', 'guessed good', 'habit id', 'hand monday', 'happy searched', 'hatehatehate microwavable', 'healthiest type', 'heaven accepted', 'helping tummy', 'higher avail', 'hold sort', 'honey mostly', 'horseradish1', 'hover immediately', 'hrefhttpwwwamazoncomgpproductb001crawcqilly', 'human foodmeat', 'ice black', 'illegal last', 'implies yet', 'included strawberry', 'individual metal', 'ingredient disqualify', 'inspired super', 'intention sounded', 'ireland spending', 'itchy', 'jalepenolol love', 'jersey taste', 'juice spill', 'keaton say', 'kibble bad', 'kinda shape', 'know gooseberry', 'ky diagnosed', 'largas', 'latte still', 'leaf bread', 'led order', 'let would', 'lifetime curse', 'like hypertension', 'likely worst', 'liquid placing', 'little lozenge', 'localhee hee', 'look deliscious', 'lot assure', 'love office', 'lowcal snack', 'macadamia several', 'magma ready', 'make stopped', 'maneuvering', 'marinade basically', 'massive trembling', 'maybe pick', 'measured make', 'mellow soy', 'mess soda', 'midst', 'mill shredded', 'mint tone', 'mistitled dont', 'mmmmm texture', 'moment relax', 'morning earthy', 'mouththroat long', 'mucusy liquid', 'mustard bad', 'natural grocersvitamin', 'need equivalent', 'never adverse', 'nice book', 'noise something', 'normal floor', 'notice total', 'nut noodle', 'oatmeal container', 'odor tell', 'oil coom', 'old nasty', 'one latter', 'oomph tea', 'optionjack', 'ordering finished', 'orleans miss', 'outstanding buy', 'overthemoon woodgrain', 'pack theyre', 'packet divided', 'pan package', 'part pulled', 'pastacheesenew', 'peaceofminddestroyers', 'people liar', 'perfect purchase', 'pertain toy', 'pickier one', 'pilot hoping', 'plain onion', 'please assured', 'pod emptyits', 'pop appreciated', 'positive quick', 'pouchesall themi', 'prairie matched', 'prepare enjoyed', 'pretzel hrefhttpwwwamazoncomgpproductb000g7tc38snyders', 'pricey toasted', 'problem rat', 'product granule', 'prolactin', 'proven unsurpassed', 'pumpkin crop', 'purchasing caused', 'putting carnation', 'questioned iodine', 'rage', 'rate may', 'read case', 'really item', 'received turned', 'recommend wholey', 'refill cap', 'regular option', 'remarkably quick', 'replace plus', 'resemblance whatsoever', 'result roasting', 'reviewed enjoy', 'rid salt', 'rit havent', 'roo ordered', 'rugrats type', 'safely recommended', 'salt activated', 'sandwich mixing', 'sauce thinly', 'say gary', 'school uk', 'search food', 'see christmas', 'seems justice', 'semimashed style', 'seriously plasticlike', 'seven newer', 'shared uh', 'shipment end', 'short travel', 'sickeningly', 'simple really', 'sipping whiskey', 'skeptical starbucks', 'slightly teaflavored', 'smell barbecue', 'smores awful', 'soaking raw', 'sold kashi', 'sometimes decaf', 'sort fakey', 'soy soyallergic', 'speed 62', 'spite year', 'spring poland', 'stale oat', 'starsbutthey taste', 'stay trader', 'stick trader', 'stock remember', 'store scoop', 'stress alimony', 'stuff biscuit', 'subsitutei', 'sugar combo', 'summary would', 'supplier food', 'surprise involved', 'sweet reconstituted', 'switched drop', 'taco salsa', 'tandoori chicken', 'taste marvelous', 'tasting four', 'tea filled', 'teeccino thought', 'terephthalate also', 'texturelook piece', 'theory amount', 'thin german', 'thinking possibly', 'thought ziti', 'tiding', 'timothy others', 'toddler house', 'tonsil wait', 'tossed thats', 'tract many', 'tread shopping', 'tried brandflavor', 'true hit', 'tryed gloria', 'turn pro', 'twodays', 'unbreakable packaging', 'unique almond', 'unused still', 'use b60', 'used roughly', 'usually sold', 'variety healthwise', 'vendor intended', 'viewed drink', 'volume volume', 'walnut half', 'warning tooth', 'water litre', 'wayne utah', 'weigh 1215', 'wellliked give', 'wheat yeah', 'wholefoods sell', 'winter made', 'wolfsize', 'work associated', 'worry tummy', 'wouldnt without', 'xp excuse', 'yes 5g', 'youre certain', 'zero doesnt']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect_TfIdf.get_feature_names()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(\"Every 1,000th feature:\\n{}\".format(feature_names[::10000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Transform data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=vect_TfIdf.transform(df_reviews_p_l['Summary_text'])\n",
    "Y_train=df_reviews_p_l['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X_Train: (426340, 3834447)\n",
      "Size of Y_Train: (426340,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of X_Train: {}\".format(X_train.shape))\n",
    "print(\"Size of Y_Train: {}\".format(Y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Run Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(LogisticRegression(), X_train, Y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.79\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) N-gram 2 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using preprocesed with Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit data into bag of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_TfIdf = TfidfVectorizer(ngram_range=(2,2)).fit(df_reviews_p_s['Summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 3125304\n",
      "Every 10,000th feature:\n",
      "['00 10', '125 felt', '1870 primari', '24 adult', '34th recip', '500 cool', '7g 2g', 'absorb sometim', 'activ herb', 'addict open', 'advis satisfi', 'ago mealtim', 'alimentum wife', 'alon sold', 'altern next', 'amazon hakubaku', 'anayway alway', 'anxieti never', 'apparantli good', 'area bottl', 'arriv vacuumpack', 'assum wonka', 'avail german', 'awesom nabisco', 'bacon smell', 'bait past', 'bar qualityi', 'bat mapl', 'beat price1', 'beguil amazon', 'besid bit', 'better splenda', 'biochem either', 'bitter groceri', 'blind robbin', 'boldbut littl', 'bottl hair', 'bowl start', 'brand extravirgin', 'breastfe result', 'broke squeez', 'buddi fed', 'burrito plain', 'buy pearson', 'cakey sort', 'came somehow', 'cannist keep', 'card go', 'case evian', 'caus stress', 'certainli variat', 'cheap amazingli', 'chef inspir', 'chicken overcook', 'chip vomit', 'cholesterol recheck', 'cinnamoni heavi', 'clearli goodfory', 'coat ingredi', 'coffe skim', 'color usual', 'common drink', 'complaint welcom', 'condit discov', 'constantli took', 'contracept yet', 'cooki peddler', 'cost honestli', 'coupl visit', 'crap legal', 'creme coconut', 'crunchier competit', 'curacao use', 'daili larg', 'daughter type', 'decad dark', 'definit hous', 'delish short', 'descript least', 'dha product', 'differ hesit', 'direct special', 'discrimin kitti', 'distributor doesnt', 'dollar gold', 'doug graham', 'dri slow', 'drown togeth', 'earlier edward', 'eat old', 'effect process', 'els furnitur', 'energ like', 'enough stopgap', 'especi help', 'even heard', 'everyon practic', 'except bbq', 'expens look', 'extract fl', 'faint heartbut', 'fantast interest', 'fathom anyon', 'feed feel', 'fibromyalgia result', 'find cv', 'firm seem', 'flake work', 'flavori havent', 'flu product', 'food mj', 'formula 99', 'fqavor product', 'fresh clean', 'frost flower', 'fullraw allnatur', 'garden brought', 'georgia pick', 'ghee world', 'giveaway damn', 'gnome isbn10', 'gone question', 'gooey chicken', 'grade matcha', 'grave unbeknownst', 'greatt common', 'groggi slight', 'guiltless pint', 'half definit', 'happen sale', 'hate brussel', 'healthi orchid', 'heavier commerci', 'hey begger', 'hit guy', 'honestli contact', 'hot better', 'howev split', 'human believ', 'icei ad', 'im san', 'inadequ descript', 'indistinguish krispi', 'ingredi select', 'instead nutrasweet', 'introduc gumbo', 'ita well', 'jalapenocheddar ole', 'jitter littl', 'junk melt', 'kept seizur', 'kind jampure', 'know disrupt', 'label confirm', 'lasagna wait', 'lba highli', 'left glum', 'let friend', 'light confect', 'like orrder', 'link salti', 'littl joke', 'locat uptown', 'look tray', 'love cod', 'lower ounc', 'mack toffe', 'major flavor', 'maluku island', 'margin help', 'matcha caramel', 'md drinker', 'medicin even', 'merci nonetheless', 'middleclass babi', 'min add', 'minut stow', 'mix uv', 'mom absolutli', 'morn job', 'movi still', 'multiyear search', 'name speed', 'necessit sprint', 'never hearti', 'nice pu', 'nonmeat part', 'noth chees', 'nut odor', 'object cut', 'offic flavorwis', 'okara add', 'one crisper', 'oogl coffe', 'order agit', 'origin cadburi', 'outsid fenc', 'overwhelm clove', 'packag east', 'paleo allnatur', 'part housewarm', 'pasteur chees', 'pear broccolilici', 'peppermint watermelon', 'perman post', 'pick enough', 'pinch ground', 'plant broken', 'plu 6th', 'pomegranatetangerin also', 'portion lesser', 'pound caraway', 'prefer fanci', 'pressur reduc', 'pricey greatr', 'proceed vomitingdiarrhea', 'product unadulter', 'protein comfort', 'pump great', 'purifi citi', 'qualiti store', 'quit star', 'raspberri freez', 'read job', 'reason exposur', 'recogn silver', 'refil necessari', 'rehydr product', 'remov fabric', 'research lowest', 'retail prompt', 'rice found', 'rip cant', 'rollup tend', 'rum 23', 'said oatswhich', 'salti suspicion', 'sauc classic', 'say equat', 'scoop wa', 'seawe snack', 'seem immun', 'send shown', 'server thai', 'shampoo past', 'shini also', 'shot 20', 'sighthound need', 'sinc news', 'size highbut', 'slightli doughi', 'smell fresher', 'snack tortilla', 'soft gentl', 'someth find', 'sorri coz', 'soybutt allerg', 'sphere bring', 'spoon command', 'squishi doar', 'starbuck inconsist', 'steak store', 'still float', 'storag least', 'strawberryseem multiberrycherri', 'stuf gummi', 'substitut dorito', 'sugarfre crystal', 'superior replac', 'sure wort', 'sweet spreadabl', 'syrup tang', 'taken common', 'tast expirement', 'tastier better', 'tealici treat', 'tend wear', 'thai also', 'thermo heat', 'thing thin', 'thoughalong aforement', 'thump nois', 'tin thought', 'togeth two', 'top drawer', 'town white', 'tray ad', 'tri hydrat', 'true kefir', 'turmer may', 'type campbel', 'understand crumbl', 'unlik stick', 'ur small', 'use nutrish', 'valu design', 'veget hate', 'vet shriek', 'vodka lighten', 'walnut moist', 'wasnt purpos', 'way crunch', 'week truck', 'wellround typic', 'whether breakfast', 'wife tempt', 'without bagel', 'worcestershir prawn', 'worst campbel', 'wrap thick', 'year 1star', 'yolk drop', 'yummo delish']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect_TfIdf.get_feature_names()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(\"Every 10,000th feature:\\n{}\".format(feature_names[::10000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Transform data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=vect_TfIdf.transform(df_reviews_p_s['Summary_text'])\n",
    "Y_train=df_reviews_p_s['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X_Train: (426340, 3125304)\n",
      "Size of Y_Train: (426340,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of X_Train: {}\".format(X_train.shape))\n",
    "print(\"Size of Y_Train: {}\".format(Y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Run Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(LogisticRegression(), X_train, Y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.77\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using preprocesed with Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit data into bag of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_TfIdf = TfidfVectorizer(ngram_range=(2,2)).fit(df_reviews_p_l['Summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 3125304\n",
      "Every 10,000th feature:\n",
      "['00 10', '125 felt', '1870 primari', '24 adult', '34th recip', '500 cool', '7g 2g', 'absorb sometim', 'activ herb', 'addict open', 'advis satisfi', 'ago mealtim', 'alimentum wife', 'alon sold', 'altern next', 'amazon hakubaku', 'anayway alway', 'anxieti never', 'apparantli good', 'area bottl', 'arriv vacuumpack', 'assum wonka', 'avail german', 'awesom nabisco', 'bacon smell', 'bait past', 'bar qualityi', 'bat mapl', 'beat price1', 'beguil amazon', 'besid bit', 'better splenda', 'biochem either', 'bitter groceri', 'blind robbin', 'boldbut littl', 'bottl hair', 'bowl start', 'brand extravirgin', 'breastfe result', 'broke squeez', 'buddi fed', 'burrito plain', 'buy pearson', 'cakey sort', 'came somehow', 'cannist keep', 'card go', 'case evian', 'caus stress', 'certainli variat', 'cheap amazingli', 'chef inspir', 'chicken overcook', 'chip vomit', 'cholesterol recheck', 'cinnamoni heavi', 'clearli goodfory', 'coat ingredi', 'coffe skim', 'color usual', 'common drink', 'complaint welcom', 'condit discov', 'constantli took', 'contracept yet', 'cooki peddler', 'cost honestli', 'coupl visit', 'crap legal', 'creme coconut', 'crunchier competit', 'curacao use', 'daili larg', 'daughter type', 'decad dark', 'definit hous', 'delish short', 'descript least', 'dha product', 'differ hesit', 'direct special', 'discrimin kitti', 'distributor doesnt', 'dollar gold', 'doug graham', 'dri slow', 'drown togeth', 'earlier edward', 'eat old', 'effect process', 'els furnitur', 'energ like', 'enough stopgap', 'especi help', 'even heard', 'everyon practic', 'except bbq', 'expens look', 'extract fl', 'faint heartbut', 'fantast interest', 'fathom anyon', 'feed feel', 'fibromyalgia result', 'find cv', 'firm seem', 'flake work', 'flavori havent', 'flu product', 'food mj', 'formula 99', 'fqavor product', 'fresh clean', 'frost flower', 'fullraw allnatur', 'garden brought', 'georgia pick', 'ghee world', 'giveaway damn', 'gnome isbn10', 'gone question', 'gooey chicken', 'grade matcha', 'grave unbeknownst', 'greatt common', 'groggi slight', 'guiltless pint', 'half definit', 'happen sale', 'hate brussel', 'healthi orchid', 'heavier commerci', 'hey begger', 'hit guy', 'honestli contact', 'hot better', 'howev split', 'human believ', 'icei ad', 'im san', 'inadequ descript', 'indistinguish krispi', 'ingredi select', 'instead nutrasweet', 'introduc gumbo', 'ita well', 'jalapenocheddar ole', 'jitter littl', 'junk melt', 'kept seizur', 'kind jampure', 'know disrupt', 'label confirm', 'lasagna wait', 'lba highli', 'left glum', 'let friend', 'light confect', 'like orrder', 'link salti', 'littl joke', 'locat uptown', 'look tray', 'love cod', 'lower ounc', 'mack toffe', 'major flavor', 'maluku island', 'margin help', 'matcha caramel', 'md drinker', 'medicin even', 'merci nonetheless', 'middleclass babi', 'min add', 'minut stow', 'mix uv', 'mom absolutli', 'morn job', 'movi still', 'multiyear search', 'name speed', 'necessit sprint', 'never hearti', 'nice pu', 'nonmeat part', 'noth chees', 'nut odor', 'object cut', 'offic flavorwis', 'okara add', 'one crisper', 'oogl coffe', 'order agit', 'origin cadburi', 'outsid fenc', 'overwhelm clove', 'packag east', 'paleo allnatur', 'part housewarm', 'pasteur chees', 'pear broccolilici', 'peppermint watermelon', 'perman post', 'pick enough', 'pinch ground', 'plant broken', 'plu 6th', 'pomegranatetangerin also', 'portion lesser', 'pound caraway', 'prefer fanci', 'pressur reduc', 'pricey greatr', 'proceed vomitingdiarrhea', 'product unadulter', 'protein comfort', 'pump great', 'purifi citi', 'qualiti store', 'quit star', 'raspberri freez', 'read job', 'reason exposur', 'recogn silver', 'refil necessari', 'rehydr product', 'remov fabric', 'research lowest', 'retail prompt', 'rice found', 'rip cant', 'rollup tend', 'rum 23', 'said oatswhich', 'salti suspicion', 'sauc classic', 'say equat', 'scoop wa', 'seawe snack', 'seem immun', 'send shown', 'server thai', 'shampoo past', 'shini also', 'shot 20', 'sighthound need', 'sinc news', 'size highbut', 'slightli doughi', 'smell fresher', 'snack tortilla', 'soft gentl', 'someth find', 'sorri coz', 'soybutt allerg', 'sphere bring', 'spoon command', 'squishi doar', 'starbuck inconsist', 'steak store', 'still float', 'storag least', 'strawberryseem multiberrycherri', 'stuf gummi', 'substitut dorito', 'sugarfre crystal', 'superior replac', 'sure wort', 'sweet spreadabl', 'syrup tang', 'taken common', 'tast expirement', 'tastier better', 'tealici treat', 'tend wear', 'thai also', 'thermo heat', 'thing thin', 'thoughalong aforement', 'thump nois', 'tin thought', 'togeth two', 'top drawer', 'town white', 'tray ad', 'tri hydrat', 'true kefir', 'turmer may', 'type campbel', 'understand crumbl', 'unlik stick', 'ur small', 'use nutrish', 'valu design', 'veget hate', 'vet shriek', 'vodka lighten', 'walnut moist', 'wasnt purpos', 'way crunch', 'week truck', 'wellround typic', 'whether breakfast', 'wife tempt', 'without bagel', 'worcestershir prawn', 'worst campbel', 'wrap thick', 'year 1star', 'yolk drop', 'yummo delish']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect_TfIdf.get_feature_names()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(\"Every 10,000th feature:\\n{}\".format(feature_names[::10000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Transform data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=vect_TfIdf.transform(df_reviews_p_l['Summary_text'])\n",
    "Y_train=df_reviews_p_s['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X_Train: (426340, 3125304)\n",
      "Size of Y_Train: (426340,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of X_Train: {}\".format(X_train.shape))\n",
    "print(\"Size of Y_Train: {}\".format(Y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Run Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/amit/anaconda3/envs/F21TA/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(LogisticRegression(), X_train, Y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.77\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
